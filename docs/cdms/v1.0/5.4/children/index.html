<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>CDMS Governance</title>
</head>

<style>
    #wrapper {
        width: 1600px;
        margin: 0 auto;
        padding: 0;
    }

    .content {
        width: 100%;
        overflow: hidden;
        overflow-wrap: break-word;
    }

    table {
        border-collapse: collapse;
        margin-bottom: 30px;
    }

    td {
        border: 1px solid black;
        padding: 0 15px;
    }

    .chapter-title {
        color: #003869;
        border-bottom: 5px solid #a9b4cf;
        padding-bottom: 10px;
    }

    .section-title {
        color: #003869;
    }

    .sub-section-title {
        color: #7489af;
    }

    pre, .cdms-content {
        white-space: pre-wrap;
        white-space: -moz-pre-wrap;
        white-space: -o-pre-wrap;
        word-wrap: break-word;
    }
</style>
<body>
<div id="wrapper">
    <div class="content">
        <table style="width: 100%;">
            <colgroup>
                <col span="1" style="width: 15%;">
                <col span="1" style="width: 70%;">
                <col span="1" style="width: 15%;">
            </colgroup>
            <tbody>
            <tr>
                <td colspan="3">
                    <h3 class="section-title"><span
                            style="padding-right: 20px">5.4</span> Quality assessment</h3>
                </td>
            </tr>
            <tr>
                <td colspan="3">
                    <h4 class="sub-section-title"><span
                            style="padding-right: 10px">5.4.1</span> Observations quality assessment
                    </h4>
                    <div class="cdms-content" style="padding-bottom: 10px;">This subsection refers to the processes implemented to help NMHSs assess the quality of observations used by their organization. It covers all stages, from the observation site and expertise level of personnel to the final product distributed to users.

The aim of this subsection is to move towards a more objective way of defining the quality of observations data.</div>
                </td>
            </tr>
            <tr>
                <td style="background-color: #dff2fd;">5.4.1.1 Siting classification</td>
                <td class="cdms-content">
                    <p>This component refers to the processes, software, governance mechanisms and analysis that classify sensors according to the rating scale described in the <em>Guide to Meteorological Instruments and
Methods of Observation</em> (WMO-No. 8), Annex 1.B Siting classifications for surface observing stations on land.</p>
                </td>
                <td style="background-color: #f3eb72;">Required</td>
            </tr>
            <tr>
                <td style="background-color: #dff2fd;">5.4.1.2 Sustained performance classification</td>
                <td class="cdms-content">
                    <p>This component refers to the processes, software, governance mechanisms and analysis that classify sensors according to their sustained performance over time.</p>
<p>The best description found to date on how to determine this classification may be found in Annex III of the final report of the first session of the Commission for Instruments and Methods of Observation Expert Team on Standardization.</p>
<p>Note: A more objective approach to developing this classification for the global WMO community is required.</p>
                </td>
                <td style="background-color: #b3d386;">Recommended</td>
            </tr>
            <tr>
                <td style="background-color: #dff2fd;">5.4.1.3 Multilayer quality flags</td>
                <td class="cdms-content">
                    <p>This component refers to the processes, software, governance mechanisms and data analysis used to understand and enumerate the quality flags of a specific record of data.</p>
<p>This will facilitate:
1. Future analysis that requires data of a specific quality flag value.
2. Communication on the assessed quality of records.</p>
<p>The best description to date on how to define this classification may be found in the <em>Guide to Climatological Practices</em> (WMO-No. 100), pp. 3–8 to 3–9. This reference describes a way of flagging quality based on a combination of:</p>
<ol>
<li>Data type (original, corrected, reconstructed or calculated)</li>
<li>Validation stage</li>
<li>Acquisition method</li>
</ol>
<p>This approach is still quite limited. It does not provide a clear way of determining just what level of quality control a record has been subjected to.</p>
<p>While the classifications are relevant and relate to the perceived quality of a record, they do not allow for an explicit comparison of data of similar perceived quality.</p>
<p>For example, the subsection on quality management (5.3.1) describes a series of classifications of tests (without providing actual details). If a record has passed all such tests, can it be considered to be better quality than one that has not passed any test?</p>
<p>Objective quality classifications are required to support a consistent approach within the global WMO community so that data can be:</p>
<ol>
<li>Objectively compared to ensure that data of similar quality can be compared and analysed as required.</li>
<li>Stored and easily retrieved from a climate database. It is becoming increasingly apparent that organizations will need to retain observations at multiple levels of quality from the raw observation through various edit and analysis processes in order to demonstrate the true lineage of a record and explain and justify the changes made to the raw observations.</li>
</ol>
<p>Note: A more objective approach to determining this classification for the global WMO community is required.</p>
                </td>
                <td style="background-color: #f3eb72;">Required</td>
            </tr>
            <tr>
                <td style="background-color: #dff2fd;">5.4.1.4 Climate observation quality classification</td>
                <td class="cdms-content">
                    <p>This component refers to the processes, software, governance mechanisms and data analysis used to understand and enumerate the quality of a specific record of data relative to an objective index. This index will need to combine a number of criteria relevant to data reliability and quality.</p>
<p>Note: This index has yet to be created. For the purposes of this publication, it is called the climate observation quality classification. However, this name may change. It is envisioned that this index will need to take into account a number of factors, including:
1. Siting classification
2. Sustained performance classification
3. Regular maintenance and calibration of sensor
4. Sensor reliability
5. Uncertainty inherent in observations
6. Observation quality control processes
7. Multilayer quality flags
8. Lineage
9. Homogeneity
10. Other appropriate factors</p>
<p>See also the summary of findings of the seventh Data Management Workshop of the European Climate Support Network (ECSN) held at the Danish Meteorological Institute, in particular:</p>
<blockquote>
<p>Noting that “everybody” talks about different levels of Quality Control [QC] and (almost) nobody uses the same wording or nomenclature – it is recommended that an overview of QC nomenclature in ECSN is worked out. It might be considered if such an overview could form the basis for a recommended set of QC wordings. (Kern- Hansen, 2009)</p>
</blockquote>
                </td>
                <td style="background-color: #e3e3e3;">Optional</td>
            </tr>
            <tr>
                <td colspan="3">
                    <h4 class="sub-section-title"><span
                            style="padding-right: 10px">5.4.2</span> Derived-data quality assessment
                    </h4>
                </td>
            </tr>
            <tr>
                <td style="background-color: #dff2fd;">5.4.2.1 Derived-data quality assessment</td>
                <td class="cdms-content">
                    <p>This component refers to the processes, software, governance and data analysis processes used to understand and enumerate the quality of derived data relative to an objective index.</p>
<p>There are many factors that can influence the quality of derived data. Some issues to consider are:
1. What is the quality of the source data?
2. What algorithms have been applied to the source data to arrive at the derived data?
3. What is the impact of these algorithms on the quality of the derived data?
4. If the derived dataset is spatial, how has the positional location of the data been derived?
    1. What is the quality of the source spatial data?
    2. What is the impact of the algorithms used to spatially distribute the data on the positional accuracy of the derived data?</p>
<p>For more information, see also the Derived data component (5.4.4.2).</p>
<p>Note: This index has yet to be created. For the purposes of this publication, it is called the derived-data quality assessment. However, this name may change.</p>
                </td>
                <td style="background-color: #e3e3e3;">Optional</td>
            </tr>
            <tr>
                <td colspan="3">
                    <h4 class="sub-section-title"><span
                            style="padding-right: 10px">5.4.3</span> Quality assurance metrics
                    </h4>
                </td>
            </tr>
            <tr>
                <td style="background-color: #dff2fd;">5.4.3.1 Quality assurance metrics</td>
                <td class="cdms-content">
                    <p>This component refers to the processes, software, governance mechanisms and analysis used to monitor the performance of quality assurance processes.</p>
<p>Such monitoring will allow network managers and climate data specialists to validate the performance of quality assurance software and processes.</p>
<p>This can be done, for example, by reviewing automatically generated reports that:
1. Summarize observational errors detected by each quality assurance test.
2. Summarize false positives and valid errors detected.
3. Compare the performance of current quality assurance metrics with historical averages.</p>
<p>These types of metrics can also help data and network managers improve quality assurance processes and software.</p>
                </td>
                <td style="background-color: #b3d386;">Recommended</td>
            </tr>
            <tr>
                <td colspan="3">
                    <h4 class="sub-section-title"><span
                            style="padding-right: 10px">5.4.4</span> Uncertainty
                    </h4>
                    <div class="cdms-content" style="padding-bottom: 10px;">This subsection refers to the processes, software, governance processes and data analysis used to understand and record the uncertainty inherent in the data.

As noted in the OGC Abstract Specification: Geographic Information – Observations and measurements (p. 13), all observations have an element of uncertainty:

&gt; The observation error typically has a systematic component, which is similar for all estimates made using the same procedure, and a random component, associated with the particular application instance of the observation procedure. If potential errors in a property value are important in the context of a data analysis or processing application, then the details of the act of observation which provided the estimate of the value are required.

This functionality will support:
1. Future statistical analysis that takes into account the uncertainty inherent in data.
2. Communication of data uncertainty.

For more information, see Wikipedia articles on:
a) Uncertain data
b) Uncertainty</div>
                </td>
            </tr>
            <tr>
                <td style="background-color: #dff2fd;">5.4.4.1 Measurements</td>
                <td class="cdms-content">
                    <p>This component refers to the processes, software, governance mechanisms and data analysis used to understand and record the uncertainty inherent in observation measurements and processes.</p>
<p>The <em>Guide to Meteorological Instruments and Methods of Observation</em> (WMO-No. 8) provides a number of examples per meteorological variable.</p>
<p>For more information, see:
a) <em>Guide to Meteorological Instruments and Methods of Observation</em> (WMO-No. 8), Annex 1.D Operational measurement uncertainty requirements and instrument performance
b) Annex III of the final report of the first session of the Commission for Instruments and Methods of Observation Expert Team on Standardization</p>
                </td>
                <td style="background-color: #f3eb72;">Required</td>
            </tr>
            <tr>
                <td style="background-color: #dff2fd;">5.4.4.2 Derived data</td>
                <td class="cdms-content">
                    <p>This component refers to the processes, software, governance mechanisms and data analysis used to understand and record the uncertainty inherent in gridded data that have been derived from observation data.</p>
<p>Many factors can contribute to the uncertainty inherent in gridded derived data. Some examples are:
1. Uncertainty inherent in the source observations data.
2. Uncertainty inherent in the location of sensors/stations used to generate the grids.
3. The relative accuracy of the algorithms used to generate the derived data.
4. The precision of variable data types used in the software that generates derived data.</p>
<p>It is also worth noting that a number of these factors may propagate through the data derivation process.</p>
                </td>
                <td style="background-color: #e3e3e3;">Optional</td>
            </tr>
            </tbody>
        </table>
    </div>
</div>
</body>
</html>