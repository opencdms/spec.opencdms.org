{
  "title": "Quality assessment",
  "subsections": [
    "5.4.1",
    "5.4.2",
    "5.4.3",
    "5.4.4"
  ],
  "components": {
    "5.4.1": [
      "5.4.1.1",
      "5.4.1.2",
      "5.4.1.3",
      "5.4.1.4"
    ],
    "5.4.2": [
      "5.4.2.1"
    ],
    "5.4.3": [
      "5.4.3.1"
    ],
    "5.4.4": [
      "5.4.4.1",
      "5.4.4.2"
    ]
  },
  "copyright": "World Meteorological Organization, 2014",
  "reference": "WMO-No. 1131",
  "version": "1.0",
  "5.4.1": {
    "title": "Observations quality assessment",
    "text": "This subsection refers to the processes implemented to help NMHSs assess the quality of\nobservations used by their organization. It covers all stages, from the observation site and\nexpertise level of personnel to the final product distributed to users.\nThe aim of this subsection is to move towards a more objective way of defining the quality of\nobservations data.",
    "5.4.1.1": {
      "title": "Siting classification",
      "text": "This component refers to the processes, software,\ngovernance mechanisms and analysis that classify\nsensors according to the rating scale described\nin the Guide to Meteorological Instruments and\nMethods of Observation (WMO-No. 8), Annex\n1.B Siting classifications for surface observing\nstations on land.",
      "classification": "Required"
    },
    "5.4.1.2": {
      "title": "Sustained performance classification",
      "text": "This component refers to the processes, software,\ngovernance mechanisms and analysis that classify\nsensors according to their sustained performance\nover time.\nThe best description found to date on how to\ndetermine this classification may be found in\nAnnex III of the final report of the first session of\nthe Commission for Instruments and Methods of\nObservation Expert Team on Standardization.\nNote: A more objective approach to developing\nthis classification for the global WMO community\nis required.",
      "classification": "Recommended"
    },
    "5.4.1.3": {
      "title": "Multilayer quality flags",
      "text": "This component refers to the processes, software,\ngovernance mechanisms and data analysis used\nto understand and enumerate the quality flags of a\nspecific record of data.\nThis will facilitate:\n\u2022 Future analysis that requires data of a specific\nquality flag value.\n\u2022 Communication on the assessed quality of\nrecords.\nThe best description to date on how to define\nthis classification may be found in the Guide to\nClimatological Practices (WMO-No. 100), pp. 3\u20138\nto 3\u20139. This reference describes a way of flagging\nquality based on a combination of:\n\u2022 Data type (original, corrected, reconstructed or\ncalculated)\n\u2022 Validation stage\n\u2022 Acquisition method\nThis approach is still quite limited. It does not\nprovide a clear way of determining just what level\nof quality control a record has been subjected to.\nWhile the classifications are relevant and relate\nto the perceived quality of a record, they do not\nallow for an explicit comparison of data of similar\nperceived quality.\nFor example, the subsection on quality\nmanagement (5.3.1) describes a series of\nclassifications of tests (without providing actual\ndetails). If a record has passed all such tests, can\nit be considered to be better quality than one that\nhas not passed any test?\nObjective quality classifications are required to\nsupport a consistent approach within the global\nWMO community so that data can be:\n\u2022 Objectively compared to ensure that data of\nsimilar quality can be compared and analysed\nas required.\n\u2022 Stored and easily retrieved from a climate\ndatabase. It is becoming increasingly\napparent that organizations will need to retain\nobservations at multiple levels of quality from\nthe raw observation through various edit and\nanalysis processes in order to demonstrate the\ntrue lineage of a record and explain and justify\nthe changes made to the raw observations.\nNote: A more objective approach to determining\nthis classification for the global WMO community\nis required.",
      "classification": "Required"
    },
    "5.4.1.4": {
      "title": "Climate observation quality classification",
      "text": "This component refers to the processes, software,\ngovernance mechanisms and data analysis used to\nunderstand and enumerate the quality of a specific\nrecord of data relative to an objective index. This\nindex will need to combine a number of criteria\nrelevant to data reliability and quality.\nNote: This index has yet to be created. For the\npurposes of this publication, it is called the climate\nobservation quality classification. However, this\nname may change.\nIt is envisioned that this index will need to take\ninto account a number of factors, including:\n\u2022 Siting classification\n\u2022 Sustained performance classification\n\u2022 Regular maintenance and calibration of sensor\n\u2022 Sensor reliability\n\u2022 Uncertainty inherent in observations\n\u2022 Observation quality control processes\n\u2022 Multilayer quality flags\n\u2022 Lineage\n\u2022 Homogeneity\n\u2022 Other appropriate factors\nSee also the summary of findings of the seventh\nData Management Workshop of the European\nClimate Support Network (ECSN) held at the\nDanish Meteorological Institute, in particular:\n> Noting that \u201ceverybody\u201d talks about\n> different levels of Quality Control [QC] and\n> (almost) nobody uses the same wording\n> or nomenclature \u2013 it is recommended that\n> an overview of QC nomenclature in ECSN\n> is worked out. It might be considered if\n> such an overview could form the basis for\n> a recommended set of QC wordings. (Kern-\n> Hansen, 2009)",
      "classification": "Optional"
    }
  },
  "5.4.2": {
    "title": "Derived-data quality assessment",
    "text": "",
    "5.4.2.1": {
      "title": "Derived-data quality assessment",
      "text": "This component refers to the processes, software,\ngovernance and data analysis processes used to\nunderstand and enumerate the quality of derived\ndata relative to an objective index.\nThere are many factors that can influence the\nquality of derived data. Some issues to consider\nare:\n\u2022 What is the quality of the source data?\n\u2022 What algorithms have been applied to the\nsource data to arrive at the derived data?\n\u2022 What is the impact of these algorithms on the\nquality of the derived data?\n\u2022 If the derived dataset is spatial, how has the\npositional location of the data been derived?\n\u2013 What is the quality of the source spatial data?\n\u2013 What is the impact of the algorithms used to\nspatially distribute the data on the positional\naccuracy of the derived data?\nFor more information, see also the Derived data\ncomponent (5.4.4.2).\nNote: This index has yet to be created. For the\npurposes of this publication, it is called the\nderived-data quality assessment. However, this\nname may change.",
      "classification": "Optional"
    }
  },
  "5.4.3": {
    "title": "Quality assurance metrics",
    "text": "",
    "5.4.3.1": {
      "title": "Quality assurance metrics",
      "text": "This component refers to the processes, software,\ngovernance mechanisms and analysis used to\nmonitor the performance of quality assurance\nprocesses.\nSuch monitoring will allow network managers\nand climate data specialists to validate the\nperformance of quality assurance software and\nprocesses.\nThis can be done, for example, by reviewing\nautomatically generated reports that:\n\u2022 Summarize observational errors detected by\neach quality assurance test.\n\u2022 Summarize false positives and valid errors\ndetected.\n\u2022 Compare the performance of current quality\nassurance metrics with historical averages.\nThese types of metrics can also help data and\nnetwork managers improve quality assurance\nprocesses and software.",
      "classification": "Recommended"
    }
  },
  "5.4.4": {
    "title": "Uncertainty",
    "text": "This subsection refers to the processes, software, governance processes and data analysis used\nto understand and record the uncertainty inherent in the data.\nAs noted in the OGC Abstract Specification: Geographic Information \u2013 Observations and\nmeasurements (p. 13), all observations have an element of uncertainty:\nThe observation error typically has a systematic component, which is similar for all estimates\nmade using the same procedure, and a random component, associated with the particular\napplication instance of the observation procedure. If potential errors in a property value are\nimportant in the context of a data analysis or processing application, then the details of the act\nof observation which provided the estimate of the value are required.\nThis functionality will support:\n\u2022 Future statistical analysis that takes into account the uncertainty inherent in data.\n\u2022 Communication of data uncertainty.\nFor more information, see Wikipedia articles on:\n\u2022 Uncertain data\n\u2022 Uncertainty",
    "5.4.4.1": {
      "title": "Measurements",
      "text": "This component refers to the processes,\nsoftware, governance mechanisms and data\nanalysis used to understand and record\nthe uncertainty inherent in observation\nmeasurements and processes.\nThe Guide to Meteorological Instruments and\nMethods of Observation (WMO-No. 8) provides a\nnumber of examples per meteorological variable.\nFor more information, see:\n\u2022 Guide to Meteorological Instruments and\nMethods of Observation (WMO-No. 8), Annex\n1.D Operational measurement uncertainty\nrequirements and instrument performance\n\u2022 Annex III of the final report of the first session\nof the Commission for Instruments and\nMethods of Observation Expert Team on\nStandardization",
      "classification": "Required"
    },
    "5.4.4.2": {
      "title": "Derived data",
      "text": "This component refers to the processes,\nsoftware, governance mechanisms and data\nanalysis used to understand and record the\nuncertainty inherent in gridded data that have\nbeen derived from observation data.\nMany factors can contribute to the uncertainty\ninherent in gridded derived data. Some examples\nare:\n\u2022 Uncertainty inherent in the source\nobservations data.\n\u2022 Uncertainty inherent in the location of\nsensors/stations used to generate the grids.\n\u2022 The relative accuracy of the algorithms used to\ngenerate the derived data.\n\u2022 The precision of variable data types used in\nthe software that generates derived data.\nIt is also worth noting that a number of these\nfactors may propagate through the data\nderivation process.",
      "classification": "Optional"
    }
  }
}